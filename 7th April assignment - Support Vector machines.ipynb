{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b0b46f1b-813e-40c0-aca3-1636cd1db76a",
   "metadata": {},
   "source": [
    "Q1. What is the relationship between polynomial functions and kernel functions in machine learning\n",
    "algorithms?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Polynomial functions and kernel functions are both mathematical tools used in machine learning, particularly in the context of kernel methods such as Support Vector Machines (SVMs) and kernelized regression techniques. They are related in the sense that kernel functions can be used to implicitly map data into a higher-dimensional feature space, and polynomial functions are one type of kernel function that can be employed for this purpose.\n",
    "\n",
    "Polynomial Kernel Function: A polynomial kernel is a type of kernel function commonly used in machine learning. It is defined as:\n",
    "\n",
    "K(x, y) = (x ⋅ y + c)^d\n",
    "\n",
    "x and y are data points.\n",
    "c is a constant.\n",
    "d is the degree of the polynomial.\n",
    "This kernel function calculates the dot product between data points x and y after transforming them into a higher-dimensional space using a polynomial function.\n",
    "\n",
    "polynomial functions are a specific type of kernel function used in kernelized machine learning algorithms to capture complex relationships in data by implicitly mapping data into a higher-dimensional space. The kernel trick enables efficient computation of these mappings, allowing for the application of kernel methods to a wide range of machine learning tasks, including classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70ac08e3-39a2-4fdb-905b-91853a837dc6",
   "metadata": {},
   "source": [
    "***********\n",
    "Q2. How can we implement an SVM with a polynomial kernel in Python using Scikit-learn?\n",
    "\n",
    "Answer : \n",
    "\n",
    "We can implement a Support Vector Machine (SVM) with a polynomial kernel in Python using Scikit-learn. Scikit-learn provides a simple and efficient interface for training and using SVMs with various kernel functions, including polynomial kernels. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7a1cc348-daa4-41ab-9939-83638e744397",
   "metadata": {},
   "outputs": [],
   "source": [
    "## importing required Scikit learn libraries \n",
    "\n",
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "\n",
    "## Create SVM classifier with a polynomial kernel\n",
    "\n",
    "# You can specify the degree of the polynomial using the 'degree' parameter\n",
    "# For example, degree=3 represents a cubic polynomial kernel\n",
    "svm_classifier = SVC(kernel='poly', degree=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b9f8034-822e-4ee6-8bbd-d82a76bd5c9d",
   "metadata": {},
   "source": [
    "This polynomial SVM_classifier can be used to train, test and predict"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40fdcfed-2cb2-4f44-bd56-62661277c90c",
   "metadata": {},
   "source": [
    "*****************\n",
    "Q3. How does increasing the value of epsilon affect the number of support vectors in SVR?\n",
    "\n",
    "Ans:\n",
    "In Support Vector Regression (SVR), the parameter epsilon (ε) controls the width of the epsilon-insensitive tube around the regression line, within which no penalty is incurred for errors. The epsilon-insensitive tube defines a region where errors smaller than ε are considered acceptable and do not contribute to the loss function used for training SVR models. However, errors larger than ε are penalized.\n",
    "\n",
    "Here's how increasing the value of epsilon affects the number of support vectors in SVR:\n",
    "\n",
    "Larger Epsilon (ε) Value: When you increase the value of epsilon, you are essentially making the epsilon-insensitive tube wider. This means that data points can be farther away from the regression line (the hyperplane in the feature space) without incurring a penalty. As a result:\n",
    "\n",
    "Fewer Support Vectors: With a larger epsilon, fewer data points will fall inside the tube or within the region where errors are larger than ε. These data points that do not contribute to the penalty for errors are called \"support vectors.\" As ε increases, more data points are no longer considered support vectors because they fall within the acceptable error region.\n",
    "\n",
    "Increased Robustness: A larger epsilon makes the SVR model more tolerant to errors and fluctuations in the data. It prioritizes finding a wider tube that encompasses more data points within the acceptable error range. This can lead to a simpler model with fewer support vectors, which may be less sensitive to noise in the data.\n",
    "\n",
    "Smaller Epsilon (ε) Value: Conversely, if you decrease the value of epsilon, you make the epsilon-insensitive tube narrower. This results in:\n",
    "\n",
    "More Support Vectors: With a smaller epsilon, more data points will be located inside the tube or within the region where errors are larger than ε. This increases the number of support vectors, as the model is more strict in enforcing that data points stay within a smaller error range.\n",
    "\n",
    "Greater Sensitivity: A smaller epsilon makes the SVR model more sensitive to individual data points, which can lead to a more complex model. While it may fit the training data more closely, it can also be more prone to overfitting and less robust to noise in the data.\n",
    "\n",
    "In summary, increasing the value of epsilon in SVR leads to a wider epsilon-insensitive tube, resulting in fewer support vectors and a more robust, less sensitive model. Conversely, decreasing epsilon leads to a narrower tube, resulting in more support vectors and a model that is more sensitive to individual data points and potentially more complex. The choice of epsilon should be based on the trade-off between model simplicity and sensitivity to data noise in your specific regression problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aff8bf1-0c82-479c-9484-392398c222b3",
   "metadata": {},
   "source": [
    "*********\n",
    "\n",
    "Q4. How does the choice of kernel function, C parameter, epsilon parameter, and gamma parameter\n",
    "affect the performance of Support Vector Regression (SVR)? Can you explain how each parameter works\n",
    "and provide examples of when you might want to increase or decrease its value?\n",
    "\n",
    "Ans:\n",
    "\n",
    "Support Vector Regression (SVR) is a powerful regression technique that relies on several parameters to control its behavior. The choice of kernel function, C parameter, epsilon parameter (ε), and gamma parameter (γ) can significantly affect the performance of SVR. Here's an explanation of each parameter and how it impacts SVR:\n",
    "\n",
    "Kernel Function:\n",
    "\n",
    "Explanation: The kernel function determines how the data is mapped from the input feature space to a higher-dimensional space. Common kernel functions include linear, polynomial, radial basis function (RBF or Gaussian), and sigmoid.\n",
    "Impact: The choice of kernel function affects the model's ability to capture complex relationships in the data. Different kernels may perform better for different types of data and regression tasks.\n",
    "Examples:\n",
    "Use a linear kernel (kernel='linear') for linear relationships between input features.\n",
    "Use a polynomial kernel (kernel='poly') for polynomial relationships with an appropriate degree.\n",
    "Use an RBF kernel (kernel='rbf') for capturing non-linear, smooth relationships.\n",
    "Experiment with different kernels based on your problem's characteristics.\n",
    "C Parameter:\n",
    "\n",
    "Explanation: The C parameter controls the trade-off between the model's complexity and the accuracy on the training data. A smaller C encourages a simpler model with a larger margin but potentially more training errors, while a larger C allows a more complex model that fits the training data closely.\n",
    "Impact: Increasing C can make the model fit the training data more closely, potentially leading to overfitting. Decreasing C can result in a more generalizable model but may underfit the data.\n",
    "Examples:\n",
    "Increase C when you suspect the model is underfitting, and you want it to fit the training data more closely.\n",
    "Decrease C when you observe overfitting, and you want the model to have a larger margin and be less sensitive to individual data points.\n",
    "Epsilon Parameter (ε):\n",
    "\n",
    "Explanation: Epsilon defines the width of the epsilon-insensitive tube around the regression line, within which errors are not penalized. It controls the trade-off between model complexity and tolerance for errors.\n",
    "Impact: A larger ε results in a wider tube, allowing more data points to be outside the tube without penalty. A smaller ε makes the model more sensitive to errors.\n",
    "Examples:\n",
    "Increase ε when you want the model to be more tolerant of errors and focus on capturing the general trend in the data.\n",
    "Decrease ε when you want the model to be less tolerant of errors and fit the training data more closely, which can lead to a narrower tube.\n",
    "Gamma Parameter (γ):\n",
    "\n",
    "Explanation: Gamma controls the shape of the RBF kernel. Higher gamma values make the kernel more sensitive to individual data points, leading to a more complex decision boundary.\n",
    "Impact: A smaller γ results in a smoother, more general decision boundary, while a larger γ can make the boundary more complex and may lead to overfitting.\n",
    "Examples:\n",
    "Decrease γ when you want a smoother decision boundary and you suspect overfitting with an RBF kernel.\n",
    "Increase γ when you want the model to be more sensitive to local variations in the data and have a more complex decision boundary.\n",
    "The choice of these parameters should be guided by cross-validation and grid search techniques to find the combination that performs best on your specific dataset. Tuning these parameters properly is crucial for achieving good SVR performance and avoiding issues like overfitting or underfitting. Additionally, the optimal parameter values can vary widely depending on the nature of your data and the regression task you are addressing."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfda4d-6c5a-4b4f-b2ed-a5d49a93cd6c",
   "metadata": {},
   "source": [
    "**********\n",
    "Q5. Assignment:\n",
    "1 Import the necessary libraries and load the dataset\n",
    "\n",
    "2 Split the dataset into training and testing set\n",
    "\n",
    "3 Preprocess the data using any technique of your choice (e.g. scaling, normaliMation\n",
    "\n",
    "4 Create an instance of the SVC classifier and train it on the training data\n",
    "\n",
    "5 hse the trained classifier to predict the labels of the testing data\n",
    "\n",
    "6 Evaluate the performance of the classifier using any metric of your choice (e.g. accuracy,\n",
    "precision, recall, F1-score\n",
    "\n",
    "7 Tune the hyperparameters of the SVC classifier using GridSearchCV or RandomiMedSearchCV to\n",
    "improve its performance\n",
    "\n",
    "8 Train the tuned classifier on the entire dataset\n",
    "\n",
    "9 Save the trained classifier to a file for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1702407d-6da5-4b69-8f56-40c26e57789c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(kernel=&#x27;poly&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "SVC(kernel='poly')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Load the Iris dataset\n",
    "iris = datasets.load_iris()\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "\n",
    "# Standardize the features to have mean=0 and variance=1\n",
    "scaler = StandardScaler()\n",
    "X_train = scaler.fit_transform(X_train)\n",
    "X_test = scaler.transform(X_test)\n",
    "\n",
    "svm_classifier = SVC(kernel='poly', degree=3)\n",
    "\n",
    "svm_classifier.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "69868115-4b63-4cf1-affe-43e77dd0d51f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = svm_classifier.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "d0bcbc3a-3873-4123-a9b4-d3a38824b2a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.96\n",
      "precision: 0.96\n",
      "recall: 0.96\n",
      "f1: 0.96\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score,precision_score, recall_score, f1_score\n",
    "\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n",
    "\n",
    "precision = precision_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "recall = recall_score(y_test, y_pred,average='weighted')\n",
    "f1  = f1_score(y_test, y_pred,average='weighted')\n",
    "\n",
    "print(f\"precision: {precision:.2f}\")\n",
    "\n",
    "print(f\"recall: {recall:.2f}\")\n",
    "\n",
    "print(f\"f1: {f1:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d726697a-b2c4-4a11-8f51-4bc5956aab54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:\n",
      "{'C': 1, 'gamma': 0.1, 'kernel': 'rbf'}\n",
      "Test Accuracy with Best Estimator: 1.00\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# Define the parameter grid for GridSearchCV\n",
    "param_grid = {\n",
    "    'C': [0.1, 1, 10],               # Regularization parameter\n",
    "    'kernel': ['linear', 'rbf'],     # Kernel type\n",
    "    'gamma': ['scale', 'auto', 0.1], # Kernel coefficient for 'rbf' kernel\n",
    "}\n",
    "\n",
    "# Create an SVC classifier\n",
    "svc_classifier1 = SVC()\n",
    "\n",
    "# Create GridSearchCV object with cross-validation (e.g., 5-fold)\n",
    "grid_search = GridSearchCV(estimator=svc_classifier1, param_grid=param_grid, cv=5, scoring='accuracy')\n",
    "\n",
    "# Fit the grid search to the training data\n",
    "grid_search.fit(X_train, y_train)\n",
    "\n",
    "# Get the best hyperparameters and estimator\n",
    "best_params = grid_search.best_params_\n",
    "best_estimator = grid_search.best_estimator_\n",
    "\n",
    "# Print the best hyperparameters\n",
    "print(\"Best Hyperparameters:\")\n",
    "print(best_params)\n",
    "\n",
    "# Evaluate the best estimator on the test data\n",
    "y_pred = best_estimator.predict(X_test)\n",
    "\n",
    "# Calculate accuracy on the test data\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Test Accuracy with Best Estimator: {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d5265ab0-343d-4116-be8c-544c9aae5c4e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved the best estimator to 'best_svm_classifier.pkl'.\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "\n",
    "pickle_file_path = 'best_svm_classifier.pkl'\n",
    "\n",
    "with open(pickle_file_path, 'wb') as file:\n",
    "    pickle.dump(best_estimator, file)\n",
    "\n",
    "print(f\"Saved the best estimator to '{pickle_file_path}'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18912b46-3c7b-4686-9aa6-eed2af41c4bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
