{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4298da54-7a1e-4555-811f-525e831d7113",
   "metadata": {},
   "source": [
    "Q1. What is Min-Max scaling, and how is it used in data preprocessing? Provide an example to illustrate its\n",
    "application.\n",
    "\n",
    "Answer: \n",
    "Min-Max scaling is a data preprocessing technique used to scale numerical features within a specific range. It transforms the original data so that all values fall within a user-defined range, usually between 0 and 1. The formula to perform Min-Max scaling on a feature is as follows:\n",
    "\n",
    "Xnew = {X - X_{{min}}}{X_{{max}} - X_{{min}}}\n",
    "\n",
    "Where:\n",
    "- X is the original value of a data point.\n",
    "- X_{{new}}  is the scaled value of the data point.\n",
    "- X_{{min}}  is the minimum value of the feature in the dataset.\n",
    "- X_{{max}}  is the maximum value of the feature in the dataset.\n",
    "\n",
    "The purpose of Min-Max scaling is to bring all features to the same scale, preventing features with larger values from dominating the learning process in machine learning algorithms that rely on distance or magnitude, such as k-nearest neighbors or gradient descent-based algorithms.\n",
    "\n",
    "Let's illustrate the application of Min-Max scaling with an example:\n",
    "\n",
    "Suppose we have a dataset containing the following numerical feature representing the age of people:\n",
    "\n",
    "Original ages: [20, 25, 30, 35, 40] \n",
    "\n",
    "We want to apply Min-Max scaling to this feature, so that all values fall within the range of 0 to 1.\n",
    "\n",
    "\n",
    "\n",
    "After applying Min-Max scaling, the transformed ages will be:\n",
    "\n",
    "Scaled ages: [0, 0.25, 0.5, 0.75, 1] \n",
    "Now, all ages fall within the range of 0 to 1, making the data ready for further analysis or machine learning algorithms that require scaled features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dfc99a19-b85b-4c7a-90ca-e3a53259adab",
   "metadata": {},
   "source": [
    "Q2. What is the Unit Vector technique in feature scaling, and how does it differ from Min-Max scaling?\n",
    "Provide an example to illustrate its application.\n",
    "\n",
    "Answer: \n",
    "The Unit Vector technique, also known as normalization, is another data preprocessing technique used to scale numerical features. Unlike Min-Max scaling, which scales features to a specific range (e.g., between 0 and 1), normalization scales the feature values such that each data point has a Euclidean norm (magnitude) of 1. It involves dividing each data point by the Euclidean norm of the entire feature vector.\n",
    "\n",
    "The formula to perform Unit Vector scaling (normalization) on a feature is as follows:\n",
    "\n",
    "Xnew = X / ||X||\n",
    "\n",
    "Where X is original value of data point \n",
    "|| X || epresents the Euclidean norm (magnitude) of the feature vector, calculated as square root of (Xi**2) \n",
    "\n",
    "Normalization is useful when we want to bring all feature vectors to the same scale and direction, making them comparable based on their direction in the feature space. It is particularly beneficial when dealing with distance-based algorithms, such as k-nearest neighbors or support vector machines.\n",
    "\n",
    "Let's illustrate the application of the Unit Vector technique (normalization) with an example:\n",
    "\n",
    "Suppose we have a dataset containing the following numerical feature representing the 2-dimensional data points:\n",
    "\n",
    "Original Data Points=[[3,4],[5,12],[8,15],[2,9]]\n",
    "\n",
    "Normalized data points: [[0.6,0.8],[0.3846,0.9231],[0.4706,0.8824],[0.2357,0.9718]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a75b6c-86ee-40c9-9e19-64e5e42e2b60",
   "metadata": {},
   "source": [
    "***************\n",
    "Q3. What is PCA (Principle Component Analysis), and how is it used in dimensionality reduction? Provide an\n",
    "example to illustrate its application.\n",
    "\n",
    "Answer: \n",
    "PCA, which stands for Principal Component Analysis, is a popular statistical technique used for dimensionality reduction in data analysis and machine learning. The primary goal of PCA is to transform a dataset containing a large number of correlated variables (features) into a new set of uncorrelated variables called principal components. These principal components are ranked by their ability to explain the variance in the data, with the first component explaining the most variance, the second component explaining the second most, and so on.\n",
    "\n",
    "The steps involved in performing PCA are as follows:\n",
    "\n",
    "Standardize the data: If the features have different scales, it is essential to standardize them (mean = 0, standard deviation = 1) to give each feature equal importance in the PCA process.\n",
    "\n",
    "Calculate the covariance matrix: The covariance matrix is computed to understand the relationships between different features and the direction and strength of their correlations.\n",
    "\n",
    "Compute eigenvectors and eigenvalues: The eigenvectors and eigenvalues are derived from the covariance matrix. Eigenvectors represent the directions (principal components), while eigenvalues represent the variance explained in those directions.\n",
    "\n",
    "Select principal components: The eigenvectors are sorted based on their corresponding eigenvalues in descending order. The principal components with the highest eigenvalues explain the most variance in the data.\n",
    "\n",
    "Project the data onto the new feature space: The original data is projected onto the new feature space formed by the selected principal components. This reduces the number of dimensions while preserving the maximum variance possible.\n",
    "\n",
    "PCA is widely used in various applications, such as data visualization, feature extraction, and noise reduction, as it can simplify complex datasets while retaining meaningful patterns and relationships.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9217dade-ff8c-42aa-93fa-4feae6fa0737",
   "metadata": {},
   "source": [
    "*******************\n",
    "Q5. You are working on a project to build a recommendation system for a food delivery service. The dataset\n",
    "contains features such as price, rating, and delivery time. Explain how you would use Min-Max scaling to\n",
    "preprocess the data.\n",
    "\n",
    "Answer:\n",
    "\n",
    "Each of the feature should follow below steps for MIN max Scaling.\n",
    "\n",
    "1.Identify the relevant features: In this case, the features we want to scale are \"price,\" \"rating,\" and \"delivery time.\" These features might have different units or scales, so it's essential to bring them to a common scale for fair comparison.\n",
    "\n",
    "2.Calculate the minimum and maximum values for each feature: Find the minimum and maximum values of \"price,\" \"rating,\" and \"delivery time\" in the dataset. These values will be used in the Min-Max scaling formula.\n",
    "\n",
    "3.Apply Min-Max scaling: For each data point, use the Min-Max scaling formula to transform the original values to their scaled counterparts.\n",
    "\n",
    "4.Update the dataset: Replace the original values of \"price,\" \"rating,\" and \"delivery time\" with their scaled values obtained from the Min-Max scaling process.\n",
    "\n",
    "By using Min-Max scaling, we ensure that all features are on the same scale, making them equally important during the recommendation process. This prevents features with larger numerical values (e.g., higher price or rating) from dominating the recommendation system and helps provide balanced and fair recommendations to users."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c8f8ea0-eed5-46e3-9fa0-878277d27fcb",
   "metadata": {},
   "source": [
    "**********************************\n",
    "Q6. You are working on a project to build a model to predict stock prices. The dataset contains many\n",
    "features, such as company financial data and market trends. Explain how you would use PCA to reduce the\n",
    "dimensionality of the dataset.\n",
    "\n",
    "Answer: \n",
    "To reduce the dimensionality of the dataset for building a model to predict stock prices, PCA (Principal Component Analysis) can be used effectively. PCA is a dimensionality reduction technique that helps identify the most significant patterns and correlations in the data and then transforms the original features into a new set of uncorrelated variables called principal components.\n",
    "\n",
    "Here's how you can use PCA to reduce the dimensionality of the dataset:\n",
    "\n",
    "Data Preprocessing: Ensure that the dataset is properly cleaned and preprocessed. Handle missing values, normalize or scale the features if necessary, and remove any irrelevant or redundant variables.\n",
    "\n",
    "Standardization: Standardize the numerical features so that they have a mean of 0 and a standard deviation of 1. This step is essential for PCA, as it is sensitive to the scale of the features.\n",
    "\n",
    "Compute the Covariance Matrix: Calculate the covariance matrix for the standardized data. The covariance matrix measures the relationships between different features and provides information about their correlations.\n",
    "\n",
    "Compute Eigenvectors and Eigenvalues: Find the eigenvectors and eigenvalues of the covariance matrix. Eigenvectors represent the principal components, and eigenvalues indicate the amount of variance explained by each principal component.\n",
    "\n",
    "Sort Eigenvectors: Sort the eigenvectors based on their corresponding eigenvalues in descending order. The eigenvector with the highest eigenvalue represents the principal component that explains the most variance in the data, followed by the second highest, and so on.\n",
    "\n",
    "Choose the Number of Principal Components: Decide on the number of principal components to retain based on the explained variance. Retaining a certain percentage of the total variance (e.g., 95% or 99%) is a common practice.\n",
    "\n",
    "Project Data onto the New Feature Space: Transform the original data by projecting it onto the selected principal components. This step reduces the number of dimensions while preserving the most significant patterns in the data.\n",
    "\n",
    "Train the Model: Use the reduced-dimensional data as input to train the predictive model. The reduced feature space should capture the essential information from the original data, helping the model make accurate predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e3892c2-389f-4cd7-a7a4-1f81c76bd765",
   "metadata": {},
   "source": [
    "****************************\n",
    "Q7. For a dataset containing the following values: [1, 5, 10, 15, 20], perform Min-Max scaling to transform the\n",
    "values to a range of -1 to 1.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "69e2b2e3-a0d0-457c-a488-1bb9de6c01cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.0, 0.21052631578947367, 0.47368421052631576, 1.0]"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ds=[1,5,10,20]\n",
    "\n",
    "Xmax=20\n",
    "Xmin=1\n",
    "\n",
    "scaled_ds=[]\n",
    "for i in ds:\n",
    "    scaled_ds.append((i-Xmin)/(Xmax-Xmin))\n",
    "\n",
    "scaled_ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d45102c0-d3ed-4bab-98f6-34263c2750fa",
   "metadata": {},
   "source": [
    "*********************\n",
    "\n",
    "Q8. For a dataset containing the following features: [height, weight, age, gender, blood pressure], perform\n",
    "Feature Extraction using PCA. How many principal components would you choose to retain, and why?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9298cfb6-261d-4db8-8c4e-e266a3fe096e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# initialize list of lists\n",
    "data = [[160, 50, 20, 0, 120], [165, 55, 22, 1, 130], [170, 80, 25, 0, 140],[150, 60, 25, 0, 130],[160, 90, 30, 0, 140]]\n",
    "  \n",
    "# Create the pandas DataFrame\n",
    "df = pd.DataFrame(data, columns=['Height', 'Weight','Age','Gender','Bp'])\n",
    "\n",
    "df_arr = np.array(df)\n",
    "\n",
    "w, v = np.linalg.eig(df_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3b3bd981-9117-4cc6-abeb-a9f2627f085c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[160,  50,  20,   0, 120],\n",
       "       [165,  55,  22,   1, 130],\n",
       "       [170,  80,  25,   0, 140]])"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8956db4c-2c9b-422e-ba98-afe8ca8e848c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([381.89133827+0.j        ,  10.02481802+0.j        ,\n",
       "        -5.1183585 +1.83124576j,  -5.1183585 -1.83124576j,\n",
       "        -1.67943929+0.j        ])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Sorting eigen values in descending order\n",
    "w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7f046da4-45af-498b-99d8-9647d65d6e20",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([381.89133827+0.j        ,  10.02481802+0.j        ,\n",
       "        -1.67943929+0.j        ,  -5.1183585 +1.83124576j,\n",
       "        -5.1183585 -1.83124576j])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.sort(w)[::-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d4c9212-d0f2-4f61-b9cb-35d4f1fffe67",
   "metadata": {},
   "source": [
    "Considering first 3 eigen values, we can consider first 3 principal components."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
