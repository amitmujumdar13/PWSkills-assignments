{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "acd55c5c-e9ab-4722-92f1-6c85a66eaad7",
   "metadata": {},
   "source": [
    "Q1. Explain the concept of R-squared in linear regression models. How is it calculated, and what does it\n",
    "represent?\n",
    "\n",
    "Answer: \n",
    "R-squared, also known as the coefficient of determination, is a statistical measure used to evaluate the goodness of fit of a linear regression model. It provides a way to assess how well the regression model explains the variability of the dependent variable (also known as the response variable) based on the independent variables (also known as the predictors or features).\n",
    "\n",
    "In a linear regression model, the goal is to find the best-fitting line (or hyperplane in higher dimensions) that minimizes the sum of the squared differences between the actual observed values and the predicted values. R-squared quantifies the proportion of the total variation in the dependent variable that is explained by the regression model. It ranges from 0 to 1, where:\n",
    "\n",
    "R-squared = 0: The regression model does not explain any of the variability in the dependent variable, and the line fits the data poorly.\n",
    "R-squared = 1: The regression model perfectly explains all the variability in the dependent variable, and the line fits the data perfectly.\n",
    "To calculate R-squared, you first need to understand the following terms:\n",
    "\n",
    "Total Sum of Squares (TSS): It represents the total variation in the dependent variable (Y) without considering the regression line. It is calculated by summing up the squared differences between each observed Y value and the mean of Y.\n",
    "\n",
    "Residual Sum of Squares (RSS): It measures the variation in the dependent variable that is not explained by the regression model. It is calculated by summing up the squared differences between each observed Y value and its corresponding predicted Y value (obtained from the regression model).\n",
    "\n",
    "The formula for R-squared is:\n",
    "\n",
    "R-squared = 1 - (RSS / TSS)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81483062-e412-41be-b03b-7c6285de2aa7",
   "metadata": {},
   "source": [
    "*************\n",
    "Q2. Define adjusted R-squared and explain how it differs from the regular R-squared.\n",
    "\n",
    "Answer: \n",
    "Adjusted R-squared is a modified version of the regular R-squared (coefficient of determination) that takes into account the number of independent variables (predictors) in the linear regression model. It is designed to address the potential issue of overfitting that can occur when using the regular R-squared, especially in models with multiple predictors.\n",
    "\n",
    "The regular R-squared measures the proportion of the total variation in the dependent variable (Y) that is explained by the regression model, without considering the number of predictors in the model. However, as more independent variables are added to the model, the R-squared value tends to increase, regardless of whether those variables are truly relevant or add any explanatory power. This can lead to the model fitting the training data too closely, potentially resulting in poor generalization to new, unseen data.\n",
    "\n",
    "The adjusted R-squared adjusts the R-squared value by penalizing the addition of unnecessary independent variables. It takes into account the number of predictors and the sample size to provide a more accurate representation of the model's goodness of fit. The formula for adjusted R-squared is:\n",
    "\n",
    "Adjusted R-squared = 1 - [(1 - R-squared) * (n - 1) / (n - k - 1)]\n",
    "\n",
    "Where:\n",
    "\n",
    "R-squared is the regular coefficient of determination.\n",
    "n is the number of data points (sample size).\n",
    "k is the number of independent variables (predictors) in the model.\n",
    "\n",
    "Here's how the adjusted R-squared differs from the regular R-squared:\n",
    "\n",
    "Penalization for additional predictors: As the number of predictors in the model increases, the adjusted R-squared will adjust downward to penalize the addition of potentially irrelevant variables. If adding an extra predictor does not significantly improve the model's fit, the adjusted R-squared will decrease, indicating that the additional variable does not contribute enough explanatory power.\n",
    "\n",
    "Interpretation with multiple predictors: In models with multiple predictors, comparing R-squared values can be misleading. The adjusted R-squared provides a more meaningful comparison as it considers the trade-off between model complexity and goodness of fit.\n",
    "\n",
    "Comparison with different sample sizes: The adjusted R-squared also allows for better comparisons of models with different sample sizes. R-squared tends to increase with larger sample sizes, even if the improvement in model fit is marginal. Adjusted R-squared accounts for this effect, providing a more consistent measure of model performance across different sample sizes.\n",
    "\n",
    "In summary, adjusted R-squared is a more conservative and reliable measure than the regular R-squared when evaluating the goodness of fit of a linear regression model with multiple predictors. It discourages the inclusion of unnecessary variables and helps in choosing a model that balances simplicity and explanatory power."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef68db74-29c5-4493-b775-df2cbbef79e9",
   "metadata": {},
   "source": [
    "****************\n",
    "\n",
    "Q3. When is it more appropriate to use adjusted R-squared?\n",
    "\n",
    "Answer: \n",
    "Adjusted R-squared is more appropriate to use in the following situations:\n",
    "\n",
    "Multiple predictors: When dealing with linear regression models that include multiple independent variables, adjusted R-squared is preferable to regular R-squared. As mentioned earlier, regular R-squared tends to increase with the addition of more predictors, even if they do not improve the model significantly. Adjusted R-squared penalizes the inclusion of irrelevant predictors, providing a more accurate assessment of the model's goodness of fit.\n",
    "\n",
    "Model comparison: When comparing different linear regression models with varying numbers of predictors, adjusted R-squared is a better metric. It helps in selecting the most parsimonious model that strikes a balance between simplicity (fewer predictors) and goodness of fit. Models with higher adjusted R-squared values are generally preferred over models with lower adjusted R-squared, as they explain more variation in the dependent variable while accounting for the complexity of the model.\n",
    "\n",
    "Sample size variation: If you are comparing models that use datasets with different sample sizes, regular R-squared may lead to misleading comparisons. Adjusted R-squared corrects for the effect of sample size on the R-squared value, making it a more suitable metric for comparing models across datasets of different sizes.\n",
    "\n",
    "Avoiding overfitting: In situations where overfitting is a concern, adjusted R-squared is particularly useful. Overfitting occurs when a model fits the training data too closely, capturing noise and idiosyncrasies rather than the true underlying patterns. Adjusted R-squared penalizes model complexity, thereby discouraging the addition of unnecessary predictors that could contribute to overfitting.\n",
    "\n",
    "Small sample size with multiple predictors: When dealing with a small sample size and a model with several independent variables, regular R-squared may yield artificially high values. This is due to the model fitting the limited data points too closely. In such cases, adjusted R-squared is a more cautious and reliable measure.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4db551e-aec9-46b8-8584-37bbfbacd536",
   "metadata": {},
   "source": [
    "*****************\n",
    "Q4. What are RMSE, MSE, and MAE in the context of regression analysis? How are these metrics\n",
    "calculated, and what do they represent?\n",
    "\n",
    "Answer: \n",
    "RMSE, MSE, and MAE are common metrics used to evaluate the performance of regression models. They assess how well the predicted values of the model align with the actual observed values of the dependent variable.\n",
    "\n",
    "1. Root Mean Squared Error (RMSE):\n",
    "RMSE is a widely used metric that measures the average magnitude of the errors (residuals) between the predicted values and the actual values. It is calculated by taking the square root of the Mean Squared Error (MSE).\n",
    "\n",
    "The formula for RMSE is as follows:\n",
    "RMSE = sqrt(MSE) = sqrt(Σ (yi - ŷi)^2 / n)\n",
    "\n",
    "Where:\n",
    "- yi represents the actual observed value of the dependent variable for the ith data point.\n",
    "- ŷi represents the predicted value of the dependent variable for the ith data point.\n",
    "- n is the number of data points.\n",
    "\n",
    "RMSE provides a measure of the typical or root-mean-square difference between the predicted and observed values. Smaller RMSE values indicate better predictive performance, as they represent a closer fit of the model to the data.\n",
    "\n",
    "2. Mean Squared Error (MSE):\n",
    "MSE is another metric that calculates the average of the squared errors (residuals) between the predicted values and the actual values. It is a useful metric as it penalizes larger errors more severely due to squaring each error.\n",
    "\n",
    "The formula for MSE is as follows:\n",
    "MSE = Σ (yi - ŷi)^2 / n\n",
    "\n",
    "Where:\n",
    "- yi represents the actual observed value of the dependent variable for the ith data point.\n",
    "- ŷi represents the predicted value of the dependent variable for the ith data point.\n",
    "- n is the number of data points.\n",
    "\n",
    "MSE is particularly useful in mathematical calculations and optimization processes because it is continuously differentiable and has desirable mathematical properties. However, its values are not as intuitive as RMSE or MAE.\n",
    "\n",
    "3. Mean Absolute Error (MAE):\n",
    "MAE is a metric that calculates the average absolute difference between the predicted values and the actual values. Unlike MSE and RMSE, it does not square the errors, making it less sensitive to outliers.\n",
    "\n",
    "The formula for MAE is as follows:\n",
    "MAE = Σ |yi - ŷi| / n\n",
    "\n",
    "Where:\n",
    "- yi represents the actual observed value of the dependent variable for the ith data point.\n",
    "- ŷi represents the predicted value of the dependent variable for the ith data point.\n",
    "- n is the number of data points.\n",
    "\n",
    "MAE represents the average magnitude of the errors, and like RMSE, smaller MAE values indicate better predictive performance, as they represent a closer fit of the model to the data.\n",
    "\n",
    "In summary:\n",
    "- RMSE and MSE emphasize larger errors due to the squaring of residuals, making them more sensitive to outliers.\n",
    "- MAE is less sensitive to outliers because it calculates the average of absolute errors.\n",
    "\n",
    "When comparing these metrics, it's important to consider the specific characteristics of the data and the goals of the regression analysis to determine which metric is most appropriate for evaluating the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67a81078-22a5-4b8f-9e67-d9f618d2cb33",
   "metadata": {},
   "source": [
    "****************\n",
    "Q5. Discuss the advantages and disadvantages of using RMSE, MSE, and MAE as evaluation metrics in\n",
    "regression analysis.\n",
    "\n",
    "Answer: \n",
    "Advantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "RMSE and MSE emphasize larger errors: These metrics penalize larger errors more severely due to the squaring of residuals. This can be beneficial when outliers or large errors are of particular concern, as the model will be encouraged to reduce their impact.\n",
    "\n",
    "Sensitivity to prediction accuracy: Both RMSE and MAE provide direct measures of prediction accuracy. Smaller values of these metrics indicate better predictive performance, as they represent a closer fit of the model to the data.\n",
    "\n",
    "Mathematical properties: MSE, in particular, has desirable mathematical properties for optimization purposes. It is continuously differentiable and convex, making it easier to work with in mathematical calculations and optimization algorithms.\n",
    "\n",
    "Interpretability: MAE and RMSE are easy to interpret, as they represent the average magnitude of the errors. This makes it easier for non-technical stakeholders to understand the model's performance.\n",
    "\n",
    "Disadvantages of RMSE, MSE, and MAE as evaluation metrics in regression analysis:\n",
    "\n",
    "RMSE and MSE sensitivity to outliers: While RMSE and MSE penalize larger errors more heavily, this sensitivity can be a disadvantage when outliers are present in the data. Outliers can significantly influence the metrics, leading to a distorted evaluation of the model's overall performance.\n",
    "\n",
    "Non-intuitive scale: MSE values are squared, which makes the units of the metric differ from the original dependent variable. This can make it difficult to interpret the magnitude of the error in real-world terms.\n",
    "\n",
    "Interpretability of MAE: While MAE is more interpretable than MSE in terms of absolute errors, it does not differentiate between overestimation and underestimation. For some applications, it might be important to consider the direction of errors.\n",
    "\n",
    "Model behavior: The choice of metric can influence the model's behavior during training. For example, models optimized with MSE might prioritize fitting large errors, potentially leading to a worse performance on average than models optimized with MAE.\n",
    "\n",
    "Overfitting: All three metrics (RMSE, MSE, and MAE) can be susceptible to overfitting, especially when the model is too complex or when hyperparameters are not properly tuned. Overfitting occurs when the model fits the training data too closely but fails to generalize well to new, unseen data.\n",
    "\n",
    "Comparison to a baseline: While these metrics provide a measure of the model's predictive performance, they do not give insight into how well the model performs compared to a baseline or a naive model.\n",
    "\n",
    "In summary, RMSE, MSE, and MAE are popular evaluation metrics in regression analysis due to their simplicity, interpretability, and sensitivity to prediction accuracy. However, they have limitations, such as sensitivity to outliers, non-intuitive scale, and possible overfitting. It's essential to consider the specific characteristics of the data and the goals of the analysis when selecting an appropriate evaluation metric. Additionally, it can be beneficial to use multiple metrics and conduct further analysis, such as residual plots, to gain a comprehensive understanding of the model's performance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1de26d29-e735-46aa-b31a-1a7ff1d4a9c3",
   "metadata": {},
   "source": [
    "*************************\n",
    "Q6. Explain the concept of Lasso regularization. How does it differ from Ridge regularization, and when is\n",
    "it more appropriate to use?\n",
    "\n",
    "Answer: \n",
    "Lasso (Least Absolute Shrinkage and Selection Operator) regularization is a technique used in linear regression and other linear models to prevent overfitting and improve model performance by adding a penalty term to the model's cost function. Lasso regularization encourages the model to reduce the impact of less relevant features by driving their corresponding regression coefficients to zero, effectively performing feature selection.\n",
    "\n",
    "The Lasso regularization adds a penalty term based on the absolute values of the regression coefficients to the original cost function. The regularization term is controlled by a hyperparameter λ (lambda). The Lasso cost function can be expressed as follows:\n",
    "\n",
    "Lasso Cost = Sum of squared errors + λ * Sum of the absolute values of the regression coefficients\n",
    "\n",
    "Mathematically, the Lasso cost function is as follows:\n",
    "\n",
    "Lasso Cost = Σ(yi - ŷi)^2 + λ * Σ|βi|\n",
    "\n",
    "Where:\n",
    "\n",
    "yi is the actual observed value of the dependent variable for the ith data point.\n",
    "ŷi is the predicted value of the dependent variable for the ith data point.\n",
    "βi is the regression coefficient for the ith feature (independent variable).\n",
    "λ is the regularization parameter that controls the strength of the penalty term.\n",
    "Differences between Lasso and Ridge regularization:\n",
    "\n",
    "Penalty term: The main difference between Lasso and Ridge regularization lies in their penalty terms. Lasso uses the L1-norm of the regression coefficients (the sum of their absolute values), while Ridge regularization uses the L2-norm (the sum of their squares).\n",
    "\n",
    "Feature selection: Lasso regularization tends to drive some regression coefficients to exactly zero, effectively performing feature selection. In contrast, Ridge regularization only penalizes the coefficients to be small, but they typically do not reach zero. This property makes Lasso more appropriate when feature selection is desired.\n",
    "\n",
    "Geometric interpretation: Geometrically, Lasso regularization tends to drive the regression coefficients to lie on the axes (axis-aligned regularization), leading to sparse solutions with some coefficients being exactly zero. On the other hand, Ridge regularization tends to drive the coefficients towards the origin, but they do not usually become zero.\n",
    "\n",
    "When is Lasso regularization more appropriate to use?\n",
    "\n",
    "Lasso regularization is more appropriate to use in the following situations:\n",
    "\n",
    "When feature selection is desired: If you have a large number of features, and you suspect that only a subset of them are relevant for the prediction task, Lasso can help identify and include only the most important features by driving the irrelevant coefficients to zero.\n",
    "\n",
    "Sparse solutions: Lasso tends to produce sparse solutions, making it beneficial when you want a simpler model with fewer predictors.\n",
    "\n",
    "Dealing with collinearity: Lasso regularization can handle multicollinearity (high correlation among predictors) better than Ridge regularization. When there are collinear features, Ridge may assign similar coefficients to those features, while Lasso may prefer one of the collinear features and set the coefficients of others to zero."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127871a5-5c29-4e25-9a76-c0e88507c418",
   "metadata": {},
   "source": [
    "*******************\n",
    "Q7. How do regularized linear models help to prevent overfitting in machine learning? Provide an\n",
    "example to illustrate.\n",
    "\n",
    "Answer: \n",
    "\n",
    "Regularized linear models help prevent overfitting in machine learning by introducing a penalty term to the model's cost function that discourages excessively large coefficient values. The penalty term is controlled by a hyperparameter that balances the trade-off between fitting the training data well and keeping the model's complexity in check. By regularizing the model, it reduces the risk of fitting noise and idiosyncrasies in the training data, leading to better generalization to new, unseen data.\n",
    "\n",
    "Here's an example to illustrate how regularized linear models prevent overfitting:\n",
    "\n",
    "Consider a dataset with a single independent variable (X) and a dependent variable (Y), and we want to build a linear regression model to predict Y based on X.\n",
    "\n",
    "Without regularization:\n",
    "In a standard linear regression model, the cost function minimizes the sum of squared errors (SSE) between the predicted values (ŷi) and the actual values (yi) of the dependent variable Y. The model tries to find the best-fitting line to minimize the SSE.\n",
    "\n",
    "Let's say we have a small dataset with five data points:\n",
    "\n",
    "X = [1, 2, 3, 4, 5]\n",
    "Y = [3, 5, 8, 10, 12]\n",
    "\n",
    "A simple linear regression without regularization may fit the data perfectly by using a higher-degree polynomial model, such as:\n",
    "\n",
    "ŷ = β0 + β1X + β2X^2 + β3X^3 + β4X^4 + β5*X^5\n",
    "\n",
    "This complex model could lead to overfitting, as it closely fits the noise and fluctuations in the small training dataset. However, when applied to new, unseen data, the model may perform poorly because it is too flexible and not capturing the underlying trend of the data.\n",
    "\n",
    "With regularization (using Lasso or Ridge):\n",
    "To prevent overfitting, we can introduce regularization using Lasso or Ridge.\n",
    "\n",
    "With Lasso regularization, the cost function adds a penalty term based on the absolute values of the regression coefficients. This encourages the model to drive some coefficients to exactly zero, effectively performing feature selection and simplifying the model.\n",
    "\n",
    "With Ridge regularization, the cost function adds a penalty term based on the squared values of the regression coefficients. This shrinks the coefficients towards zero but typically does not force any of them to become exactly zero.\n",
    "\n",
    "By introducing regularization, the complex higher-degree polynomial model will be discouraged because it results in large coefficient values. Instead, the regularized model may prefer a simpler linear model:\n",
    "\n",
    "ŷ = β0 + β1*X\n",
    "\n",
    "The simpler model is less prone to overfitting as it generalizes better to new data. Regularization helps to balance the model's ability to fit the training data well while keeping it simple and preventing overfitting.\n",
    "\n",
    "In summary, regularized linear models introduce penalty terms to the cost function, which discourages complex models with large coefficient values. This helps prevent overfitting by promoting simpler models that generalize better to new, unseen data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed0189e3-a227-4a6a-844c-01d24c21bf4c",
   "metadata": {},
   "source": [
    "****************\n",
    "Q8. Discuss the limitations of regularized linear models and explain why they may not always be the best\n",
    "choice for regression analysis.\n",
    "\n",
    "Answer: \n",
    "\n",
    "Regularized linear models, such as Lasso and Ridge regression, are powerful techniques for preventing overfitting and improving the generalization of linear models. However, they have certain limitations and may not always be the best choice for regression analysis in all scenarios. Some of the limitations include:\n",
    "\n",
    "Model interpretability: Regularized linear models can lead to sparse solutions, with some regression coefficients being exactly zero. While this sparsity is beneficial for feature selection and model simplicity, it may sacrifice interpretability in some cases. When interpretability is a primary concern, standard linear regression without regularization may be preferred, as it provides clear insights into the relationship between predictors and the dependent variable.\n",
    "\n",
    "Loss of important features: While regularization can be beneficial for discarding irrelevant features, it may also lead to the loss of some genuinely important predictors. If the true underlying relationship in the data involves multiple predictors with small coefficients, Lasso regularization, in particular, might set some of these coefficients to zero, neglecting essential information for accurate predictions.\n",
    "\n",
    "Model bias: The regularization term introduces bias into the model by shrinking the coefficients towards zero. This bias can lead to underestimating the true effects of some predictors, potentially resulting in a model that is not as accurate as a non-regularized model in certain situations.\n",
    "\n",
    "Optimizing hyperparameters: Regularized linear models have hyperparameters (such as λ for Lasso and Ridge) that need to be tuned carefully. Selecting the appropriate hyperparameters can be a challenging task and may require cross-validation or other hyperparameter tuning techniques. Improper tuning can lead to suboptimal model performance.\n",
    "\n",
    "Non-linear relationships: Regularized linear models are inherently limited to linear relationships between predictors and the dependent variable. In cases where the relationship is non-linear, regularized linear models may not capture the complex patterns effectively.\n",
    "\n",
    "Large-scale data: For very large-scale datasets, the computational cost of training regularized linear models can be substantial. In such cases, other techniques like gradient boosting or neural networks might be more efficient and equally effective for handling large and complex datasets.\n",
    "\n",
    "Multicollinearity: While Ridge regularization can handle multicollinearity well, Lasso might struggle with highly correlated predictors. In the presence of multicollinearity, Ridge regularization may be a more suitable choice."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "777c2052-5502-4a3d-9079-dc4d9ba26961",
   "metadata": {},
   "source": [
    "*******************\n",
    "\n",
    "Q9. You are comparing the performance of two regression models using different evaluation metrics.\n",
    "Model A has an RMSE of 10, while Model B has an MAE of 8. Which model would you choose as the better\n",
    "performer, and why? Are there any limitations to your choice of metric?\n",
    "\n",
    "Answer: \n",
    "To determine which model is the better performer, we need to consider the specific characteristics of the RMSE and MAE metrics and the goals of the regression analysis.\n",
    "\n",
    "RMSE (Root Mean Squared Error):\n",
    "RMSE measures the average magnitude of the errors (residuals) between the predicted values and the actual values. Smaller RMSE values indicate better predictive performance, as they represent a closer fit of the model to the data. In this case, Model A has an RMSE of 10, which means, on average, the model's predictions deviate from the actual values by approximately 10 units.\n",
    "\n",
    "MAE (Mean Absolute Error):\n",
    "MAE measures the average absolute difference between the predicted values and the actual values. Like RMSE, smaller MAE values indicate better predictive performance, as they represent a closer fit of the model to the data. In this case, Model B has an MAE of 8, which means, on average, the model's predictions deviate from the actual values by approximately 8 units.\n",
    "\n",
    "Comparison:\n",
    "Since both RMSE and MAE are measures of prediction accuracy, we can compare the two models based on their respective metrics. Model B has a smaller MAE (8) compared to Model A's RMSE (10). Generally, a lower MAE suggests that Model B's predictions are, on average, closer to the actual values compared to the average deviation between predictions and actual values in Model A.\n",
    "\n",
    "Preferred Model:\n",
    "Based on the metrics alone, we would choose Model B as the better performer since it has a lower MAE, indicating better predictive accuracy on average compared to Model A.\n",
    "\n",
    "Limitations of Metric Choice:\n",
    "While choosing the best model based on MAE or RMSE seems straightforward in this case, it's essential to consider potential limitations:\n",
    "\n",
    "Sensitivity to outliers: Both MAE and RMSE are sensitive to outliers. If the dataset contains outliers, RMSE may be influenced more heavily, as it squares the errors. Thus, a few large outliers could significantly impact the RMSE value.\n",
    "\n",
    "Weighting errors: MAE treats all errors equally, while RMSE emphasizes larger errors due to the squaring of residuals. Depending on the specific application, either metric may be preferred based on whether larger errors are more critical to minimize.\n",
    "\n",
    "Context and domain-specific factors: The choice of metric should consider the specific context of the regression analysis and the domain-specific factors that influence the importance of prediction accuracy. In some cases, the choice of metric may be influenced by business or research requirements."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b77b88-190f-4065-a499-adf8716b38ce",
   "metadata": {},
   "source": [
    "***************\n",
    "Q10. You are comparing the performance of two regularized linear models using different types of\n",
    "regularization. Model A uses Ridge regularization with a regularization parameter of 0.1, while Model B\n",
    "uses Lasso regularization with a regularization parameter of 0.5. Which model would you choose as the\n",
    "better performer, and why? Are there any trade-offs or limitations to your choice of regularization\n",
    "method?\n",
    "\n",
    "Answer: \n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
